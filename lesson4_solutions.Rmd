---
title: "Solutions for Lesson 4 - tidyverse version"
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 3
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Function to read and tidy chick data
read_chick_data <- function(file){
  
  # Read the data in the file - starts at line 3
  chick_data <- read_delim(file, skip = 2, delim = " ", col_types = "ii")
  
  # Extract the diet from the file - first line of the file
  # and remove the word "Diet:" from it
  chick_diet <- read_lines(file, n_max = 1) %>% 
    str_remove("Diet:")

  # Get the chick ID - from the file name
  chick_id <- basename(file) %>% 
    str_remove(".txt") %>% str_remove("Chick_")
  
  # Add diet and chick ID to our table
  chick_data <- chick_data %>% 
    mutate(diet = chick_diet, id = chick_id)
  
  # Return the tidy chick_data
  return(chick_data)
}
```

We were given 50 text files, each containing data for the weights of chicks under 
different diets. The format of each text file is the same:

* The name of the file gives the chick ID
* The first line in the file givees the diet ID
* Following lines contain data in tabular format with the first column giving 
the days from birth and the second column the weight of the chick.

We were asked to answer several questions from these data. But before we can 
answer them, we need to parse these files and combine them into a tidy tabular 
format. 

There are many ways to solve this exercise. Here, we show a solution using several 
functions from the `tidyverse` meta-package.

```{r, message = FALSE}
# load the package
library(tidyverse)
```


## Prelude

For any problem where we need to repeat the same task several times, it helps to 
break down the problem into parts. We can write down what the different steps 
are using words and not worrying about the code.

Here's an example for our problem of parsing and combining the data files:

* For each file make a table with 4 columns: `chick_id`, `diet`, `day`, `weight`
    * store the file name, which contains the chick ID
    * extract the first line, which contains the diet ID
    * extract lines 3 onwards, which contain the day and weight
* Repeat the steps above for all files
* Combine all tables together into a single table

Now that we've broken our problem into parts, let's tackle them in turn.

## Understanding our data

If we open one of our files in a text editor, this is how they look like:

```{r, echo=FALSE}
cat(read_lines("data/chick_data/Chick_1.txt"), sep = "\n")
```

There are 3 pieces of information that we should bring together into a single 
tidy _data frame_:

* information about the diet applied - first line of the file 
* the weights measured on each day - third line of the file onwards, formated as a 
space-delimited table
* the ID of the chick - this is contained in the file name

That is, we should aim for something like this:

```{r, echo = FALSE}
read_chick_data("data/chick_data/Chick_1.txt") %>% knitr::kable()
```


## Building the steps to read and tidy our data

We start by focusing on a single file, and then generalise across all files.

If you look at the help of `?read_delim`, you will notice that these `read_*` functions 
have an option to skip some of the lines of the file. This is very convenient, 
as it allows us to easily read lines 3 onwards, which contain the day and respective 
weight for our chick:

```{r, message=FALSE}
# Can read the table starting from line 3
read_delim("data/chick_data/Chick_1.txt", 
           delim  = " ", skip = 2)
```

So this solves one of our steps! (note that base R functions such as `read.table()` 
also have an option to skip lines of a file before reading it in)

Let's now turn into reading the first line of the file, which contains the 
diet identifier. Again, if you look at the help of `read_*` functions, you will 
notice there is an option `n_max`, which determines how many lines of the file 
we want to read in (the default is to read all lines). We can therefore use this 
option to read the first line of the file only.

We will use a function called `read_lines()`, which simply reads the lines of a 
file and stores them into a vector. This function is not often used, but can 
be useful when parsing data.

```{r}
# Read the first line of the file
read_lines("data/chick_data/Chick_1.txt", n_max = 1)
```

Because we are reading the first line of the file only, the output is a vector 
of length 1. For our final dataset, we don't really need to keep the string "Diet:"
because we will place these values in a column. Therefore, it is useful to remove 
that string from our value. We will use the function `str_remove()`, which 
we can pipe from the previous line of code:

```{r}
# Read first line and remove "Diet:" string
read_lines("data/chick_data/Chick_1.txt", n_max = 1) %>% 
  str_remove("Diet:")
```

And that's another problem solved!

Finally, we want to extract the chick ID from a file name. So far we've been 
looking at a single file, but eventually we will create a list of file names 
too  apply these steps to. For now, let's manually type the file name of the 
first file we've been experimenting on:

```{r}
"data/chick_data/Chick_1.txt"
```

From this file name, we want to extract the file name only (without the directory), 
which we can do using the `basename()` function:

```{r}
"data/chick_data/Chick_1.txt" %>% basename()
```

Then, we want to remove the string "Chick_" and ".txt" from it, which we've already 
seen can be done with `str_remove()`:

```{r}
"data/chick_data/Chick_1.txt" %>% basename() %>% 
  str_remove("Chick_") %>% str_remove(".txt")
```

And that's it!


## Putting it all together into a function

We can wrap all these steps into our own custom function. This will be convenient 
when we want to apply this function to the entire list of files.

```{r}
read_chick_data <- function(file){
  
  # Read the data in the file - starts at line 3
  ## explicitly specify that the columns are integer ("i") and numeric ("n")
  chick_data <- read_delim(file, skip = 2, delim = " ", col_types = "in")
  
  # Extract the diet from the file - first line of the file
  # and remove the word "Diet:" from it
  chick_diet <- read_lines(file, n_max = 1) %>% 
    str_remove("Diet:")

  # Get the chick ID - from the file name
  chick_id <- basename(file) %>% 
    str_remove(".txt") %>% str_remove("Chick_")
  
  # Add diet and chick ID to our table
  chick_data <- chick_data %>% 
    mutate(diet = chick_diet, id = chick_id)
  
  # Return the tidy chick_data
  return(chick_data)
}
```

Let's test the function:

```{r}
read_chick_data("data/chick_data/Chick_1.txt")
```



## Apply function to list of files

First let's create a list of data files in our directory:

```{r}
file_list <- list.files("data/chick_data/", pattern = ".txt",
                        full.names = TRUE)
head(file_list)
```

Then, we can apply a function to a list of values using the `map()` function (in 
base R there's the `lapply()` function). 

```{r, eval=FALSE}
chicks <- map(file_list, read_chick_data)
```

The result is a list object, which contains 50 data.frames.  

You can think of `map()`/`lapply()` as encapsulated _for loops_. In this case, 
it would be like writting:

```{r, eval=FALSE}
chicks <- list()
for(i in seq_along(file_list)){
  chicks[[i]] <- read_chick_data(file_list[i])
}
rm(i)
```

The difference is that these functions do not clutter your environment with intermediate 
variables (in this case, the variable `i` was created by the for loop, but no 
such variable is created by the `map()` function). It is also a much more compact 
syntax, you don't need to manually create the object that will store the results 
of the loop (in this case we have to create an empty list first). 
And finally, these functions can sometimes be faster than standard _for loops_. 

As we saw, the `map()` function returns a list. This is sometimes convenient, but in our case
we want all of these tables to be bound together into a single `data.frame`. 

We can "bind" those tables using the `bind_rows()` function:

```{r, eval=FALSE}
chicks <- bind_rows(chicks)
```

Alternatively, it's worth knowing that there are several variants of `map()`, 
which are designed for when we want the output to be of a defined type. 
In this case, we could use `map_dfr()`, which returns a `data.frame` bound by rows, 
instead of a `list`:

```{r}
# Apply our function to all of the files, and return a data.frame
chicks <- map_df(file_list, read_chick_data)

head(chicks)
```

Learn more about `map()` and its variants from the 
[iteration chapter in R for Data Science](http://r4ds.had.co.nz/iteration.html)


## Analysing our data

The rest of the exercise is now applying what we've already learned in previous 
lessons using `dplyr` functions.

#### Question 2

```{r}
# How many chicks survived the whole experiment?
## We can count the number of observations on each day
chicks %>% 
  count(Day)
```

Get IDs of surviving IDs

```{r}
# Get IDs of individuals that survived up to day 21
surviving_ids <- chicks %>% 
  group_by(id) %>% 
  filter(Day == 21) %>% 
  pull(id)  # this extracts the "id" column from the table and returns a vector

surviving_ids
```

Filter original table

```{r}
# Filter the table using this variable
my_chicks_survive <- chicks %>% 
  filter(id %in% surviving_ids)
```


#### Question 3a

```{r}
# Calculate the weight at day 21 minus the weight at day 0
wgt_dif <- my_chicks_survive %>% 
  group_by(id) %>% 
  summarise(weight_dif = Weight[Day == 21] - Weight[Day == 0])

head(wgt_dif)
```

Calculate the mean of the `weight_dif` column:

```{r}
mean(wgt_dif$weight_dif)
```


#### Question 3b

```{r}

my_chicks_survive %>% 
  # for each chick on each diet
  group_by(id, diet) %>% 
  # calculate increase in weight (between day 0 and day 21)
  summarise(weight_dif = Weight[Day == 21] - Weight[Day == 0]) %>% 
  # then for each diet
  group_by(diet) %>% 
  # calculate the mean increase in weight across these chicks
  summarise(mean_weight_dif = mean(weight_dif))
```

#### Question 3c

Recycling the code above, but piping it directly to a `ggplot2` boxplot:

```{r}
my_chicks_survive %>% 
  group_by(id, diet) %>% 
  summarise(weight_dif = Weight[Day == 21] - Weight[Day == 0]) %>% 
  ggplot(aes(diet, weight_dif)) + geom_boxplot()
```


#### Question 3d

```{r}
my_chicks_survive %>% 
  # First retain only chicks that have more than 130g of weight
  filter(Weight > 130) %>% 
  # Then for each chick ID
  group_by(id) %>% 
  # retain the row that corresponds to the first day this is weight is reached
  filter(Day == min(Day)) %>% 
  # and plot it (geom_bar automatically does counts for us)
  ggplot(aes(Day)) + geom_bar()
```


#### Question 3e

```{r}
ggplot(chicks, aes(Day, Weight, group = id, colour = diet)) +
  geom_line() +
  scale_colour_viridis_d() # colour-blind safe palette
```


#### Question 3f

```{r}
chicks %>% 
  # For each Day and diet
  group_by(Day, diet) %>% 
  # Calculate the mean weight
  summarise(mean_weight = mean(Weight)) %>% 
  # and plot it
  ggplot(aes(Day, mean_weight, colour = diet)) +
  geom_line() +
  scale_colour_viridis_d()
```

Or you can summarise data from within `ggplot2` functions directly. This is very 
useful if you want to show both summarised and original data, such as shown here:

```{r}
ggplot(chicks, aes(Day, Weight)) +
  geom_line(aes(group = id), colour = "grey") +
  geom_line(aes(colour = diet), stat = "summary", fun.y = "mean", size = 1) +
  scale_colour_viridis_d()
```


#### Question 3g

Let's first create a table of average weight per day (that will address the 
first two points in the question):

```{r}
avg_wgts <- my_chicks_survive %>% 
  group_by(Day) %>% 
  summarise(mean_wgt = mean(Weight)) %>% 
  ungroup()

head(avg_wgts)
```

Then let's create a table of average weight per day and per diet:

```{r}
avg_wgts_diet <- my_chicks_survive %>% 
  group_by(Day, diet) %>% 
  summarise(mean_wgt_diet = mean(Weight)) %>% 
  ungroup()

head(avg_wgts_diet)
```

Now, let's join these two tables together, using the "Day" column as a reference:

```{r}
avg_wgts <- full_join(avg_wgts, avg_wgts_diet, by = "Day")

head(avg_wgts)
```

So now we can calculate the mean weight per diet as a percentage of the overal 
mean weight across diets:

```{r}
avg_wgts <- avg_wgts %>% 
  mutate(pct_wgt_diet = mean_wgt_diet/mean_wgt*100)

head(avg_wgts)
```

Finally, we need to _reshape_ our table so that each diet has its own column:

```{r}
avg_wgts %>% 
  # Add a prefix "diet" for more friendly column names after spreading
  mutate(diet = paste0("diet", diet)) %>% 
  # remove this column which we do not need in the output
  select(-mean_wgt_diet) %>% 
  # spread the columns
  spread(diet, pct_wgt_diet)
```

